
<!DOCTYPE html><html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.27.0" theme-name="Stellar" theme-version="1.27.0">
  
  <meta name="generator" content="Hexo 7.2.0">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f9fafb">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000">
  
  <title>模型微调技术文档 - 个人博客</title>

  
    <meta name="description" content="大模型部署训练参考 https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1ce411J7nZ?p&#x3D;28&amp;vd_source&#x3D;5de3f4ba65a01a4fa9d447425584415b  算力消耗：训练&gt;微调&gt;推理 硬件需求：如何组件一台或多台高性能的个人计算机或服务器 两种途径：配置个人计算机或服务；租用在线gpu服务  训练模型需要GPU+计算架构，如NV">
<meta property="og:type" content="article">
<meta property="og:title" content="模型微调技术文档">
<meta property="og:url" content="http://example.com/2024/08/17/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3/index.html">
<meta property="og:site_name" content="个人博客">
<meta property="og:description" content="大模型部署训练参考 https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1ce411J7nZ?p&#x3D;28&amp;vd_source&#x3D;5de3f4ba65a01a4fa9d447425584415b  算力消耗：训练&gt;微调&gt;推理 硬件需求：如何组件一台或多台高性能的个人计算机或服务器 两种途径：配置个人计算机或服务；租用在线gpu服务  训练模型需要GPU+计算架构，如NV">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-08-17T06:41:10.000Z">
<meta property="article:modified_time" content="2024-08-17T06:41:51.648Z">
<meta property="article:author" content="wangwenqq">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
  
  
  
  <meta name="keywords" content="AI">

  <!-- feed -->
  

  <link rel="stylesheet" href="/css/main.css?v=1.27.0">

  

  

  
</head>
<body>

<div class="l_body s:aa content tech" id="start" layout="post" ><aside class="l_left"><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><a class="avatar" href="/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/images/a.jpeg" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a><a class="title" href="/"><div class="main" ff="title">个人博客</div><div class="sub cap">技术学习分享</div></a></div></header>

<div class="nav-area">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="站内搜索"></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div>


<nav class="menu dis-select"></nav>
</div>
<div class="widgets">


<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">最近更新</span></div><div class="widget-body fs14"><a class="item title" href="/2024/08/17/ollama/"><span class="title">ollama</span></a><a class="item title" href="/2024/08/17/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3/"><span class="title">模型微调技术文档</span></a><a class="item title" href="/2024/08/17/%E9%95%BF%E6%96%87%E6%9C%AC%E6%80%BB%E7%BB%93%E5%AE%9E%E7%8E%B0/"><span class="title">长文本总结实现</span></a><a class="item title" href="/2024/08/17/DDOS%E6%94%BB%E5%87%BB/"><span class="title">DDOS攻击</span></a><a class="item title" href="/2024/08/17/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6-vllm/"><span class="title">大模型加速推理框架-vllm</span></a><a class="item title" href="/2024/08/08/%E5%A4%9AAgent%E6%A1%86%E6%9E%B6-AutoGen/"><span class="title">多Agent框架-AutoGen</span></a><a class="item title" href="/2024/08/07/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"><span class="title">注意力机制</span></a><a class="item title" href="/2024/06/22/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"><span class="title">大模型安全</span></a><a class="item title" href="/2024/06/22/python-overload/"><span class="title">python-overload</span></a><a class="item title" href="/2024/06/22/%E6%B5%81%E5%BC%8F%E4%BC%A0%E8%BE%93/"><span class="title">流式传输</span></a></div></widget>
</div>

</div></aside><div class="l_main" id="main">





<div class="article banner top">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a>
<span class="sep"></span><a class="cap breadcrumb" href="/">文章</a></div>
<div class="flex-row" id="post-meta"><span class="text created">发布于：<time datetime="2024-08-17T06:41:10.000Z">2024-08-17</time></span><span class="sep updated"></span><span class="text updated">更新于：<time datetime="2024-08-17T06:41:51.648Z">2024-08-17</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>模型微调技术文档</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content"><h1 id="大模型部署训练"><a href="#大模型部署训练" class="headerlink" title="大模型部署训练"></a>大模型部署训练</h1><p>参考 <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ce411J7nZ?p=28&vd_source=5de3f4ba65a01a4fa9d447425584415b">https://www.bilibili.com/video/BV1ce411J7nZ?p=28&amp;vd_source=5de3f4ba65a01a4fa9d447425584415b</a> </p>
<p>算力消耗：训练&gt;微调&gt;推理</p>
<h2 id="硬件需求：如何组件一台或多台高性能的个人计算机或服务器"><a href="#硬件需求：如何组件一台或多台高性能的个人计算机或服务器" class="headerlink" title="硬件需求：如何组件一台或多台高性能的个人计算机或服务器"></a>硬件需求：如何组件一台或多台高性能的个人计算机或服务器</h2><ul>
<li><p>两种途径：配置个人计算机或服务；租用在线gpu服务</p>
</li>
<li><p>训练模型需要GPU+计算架构，如NVIDIA的GPU和针对其自己GPU设计的CUDA计算架构（并行计算和编程平台）</p>
</li>
<li><p>硬件配置建议：</p>
<ol>
<li><p>根据想要部署的大模型官方配置说明，先选择出最合适的GPU，再根据GPU的级别，进一步搭配计算机的其他组件，如CPU、内存、存储等</p>
<ul>
<li>匹配GPU的标准是：根据应用需求（推理还是微调），先关注显存大小，<strong>必须满足官方的最低显存要求</strong></li>
<li>主流的显卡显存容量：超算级别显卡A100、H100、H800为80G显存，其中A100也有40G显存；消费级显卡4090和3090显存为24G</li>
</ul>
</li>
<li><p>显存满足需求的前提下，根据不同显卡的性能和成本进行权衡，选择性价比最高的，即：单张高性能卡，还是多张较低版本的卡</p>
<ul>
<li>硬件组合分类：<ul>
<li>纯cpu(不推荐)</li>
<li>单机单卡：适用于大多数个人使用和中等计算负载的场景（典型配置）</li>
<li>单机多卡：适用于搞计算负载的场景（典型配置）</li>
<li>多机配置：使用多台计算机进行集群计算，通常超出个人使用范围</li>
</ul>
</li>
<li>当前市场主要两家GPU厂商，分别是NVIDIA和AMD,根据相关统计数据，在独立显卡领域，NVIDIA的市场占率高达85%，现在做深度学习、大模型，NVIDIA的卡就是刚需，基本没有其他选择。NVIDIA先后推出了V100、A100、H100多款专门用于AI训练的芯片，都是比较热门且熟知的选择。A100和H100，22年10月在中国被禁止销售，后NVIDIA推出了A800、H800,性能与A100、H100没有差别特别多，23年10月A800和H800也被禁售，目前A系列和H系列因为是断供前出现的计算芯片，国内还有货，<strong>但市场渠道比较乱，需要甄别</strong></li>
<li>计算级显卡和消费级显卡如何选择<ul>
<li>没有双精度需求，追求性价比，选择4090；有双精度需求，选A100，没有A100，选A800</li>
<li>如果做大模型训练，GeForce RTX 4090不行</li>
<li>如果做大模型推理，GeForce RTX 4090性价比方面，优于A100;</li>
<li>如果做大模型微调，最好A100,GeForce RTX 4090也可以，但是需要多卡</li>
<li>10万+的预算配置，4张4090没什么问题</li>
<li>20-30万预算可以考虑8张4090，或两张A100 80G</li>
<li>如果预算不限，A100 8卡一定是最佳的选择</li>
<li>双卡gpu升级路线：3090-&gt; 4090 -&gt; A100 40G -&gt; A100 80G</li>
<li>大型工业级实验要求：全量微调(例如chatglm36B模型)至少4张A100 80G</li>
<li><strong>用于AI深度学习或者大模型的显卡，一定要买涡轮版的</strong>：涡轮卡尾部供电，散热效果强于风扇卡；涡轮卡尺寸和高度远低于风扇卡，便于多卡安装</li>
<li>个人计算机，典型的配置是单GPU或双GPU,一般不超过4个GPU，否则机箱放不下，且运行噪声很大，且容易跳闸</li>
</ul>
</li>
</ul>
</li>
<li><p>其他配置</p>
<ul>
<li>cpu: cpu瓶颈并没有太大，<strong>一般1个GPU对应2-4个cpu核数</strong>，<strong>与选择的GPU性能水平相匹配</strong>，避免将一款高端显卡与低端cpu或一款高端cpu与低端显卡相匹配，这会导致性能瓶颈。<strong>低于Intel i5系列的，就不要考虑了</strong>，以intel举例：同代产品i7比i5强，老一代i7和新一代i5比较，就未必成比。</li>
<li>散热器：cpu散热器有两种：水冷和风冷，相对来说，水冷&gt;风冷，如果cpu有超频需求，且购买的cpu是超频版本，建议水冷，否则风冷</li>
<li>主板：根据cpu和gpu级别，<strong>低端主板不考虑</strong><ul>
<li>intel：Z系列（高端）、B系列（中端）、H系列（低端）</li>
<li>amd: X系列（高端）、B系列（中端）、A系列（低端）</li>
<li><strong>选择的cpu型号要搭配对应型号的主板</strong>，避免出现：intel cpu去搭配amd主板，反之亦然</li>
</ul>
</li>
<li>硬盘选型原则：<ul>
<li>第一步，看接口类型。主流固态硬盘主要有两种接口：SATA和M.2。<ul>
<li>SATA：体积大，速度慢，最高传输速度为600MB&#x2F;s; </li>
<li>M.2:硬盘小，采用新的硬盘协议，速度可以达到4GB&#x2F;s(推荐)</li>
</ul>
</li>
<li>第二步，看协议。M.2接口的固态硬盘分为SATA协议和NVME协议<ul>
<li>SATA协议：本质上就是采用了M.2接口的SATA盘，速度慢</li>
<li>NVME协议：速度快（推荐）</li>
</ul>
</li>
<li>第三步：看PCle等级。等级越高，传输速度越快，当前市面上最新的PCle 5.0,PCle 4.0 和PCle 3.0,一般来说选择PCle 4.0即可</li>
</ul>
</li>
<li>内存：<strong>建议内存容量是GPU显存的一到两倍即可</strong>。单卡GPU,至少16GB内存，四卡GPU,至少64GB内存；注意检查主板是否支持内存的型号及可插的槽位；<strong>注意检查cpu、主板是否执行选择的内存频率</strong>，内存的频率受限于cpu和主板的限制</li>
<li>电源：电源的瓦数要满足整机的功耗，电源的消耗大户是cpu和GPU，一个简单的方法是<strong>将CPU和GPU的TDP功耗相加，然后乘以2</strong>，例如：一个65W的CPU加上一个125W的GPU,合适的电源瓦数约为400W或450W,双卡最好买1000W以上，四卡最好买1600W的电源</li>
<li>机箱：核对主板与机箱尺寸匹配性，确保兼容，如Atx主板要搭配中塔机箱；确认机箱支持显卡尺寸；建议高出显卡长度至少30毫米；检查CPU散热器是否能安装下，如果是水冷，检查水冷冷排尺寸；检查电源和机箱的尺寸</li>
</ul>
</li>
</ol>
</li>
<li><p>租用在线GPU服务</p>
<ul>
<li>白嫖GPU平台推荐<ul>
<li>阿里云人工智能PAI:可选A10、V100、G6,可白嫖三个月<ul>
<li>PAI-DSW:实践大模型，选这个</li>
<li>PAI-EAS</li>
<li>PAI-DLC</li>
</ul>
</li>
<li>阿里云天池：A100、T4随机分配，免费60个小时，可持续获取免费时长</li>
<li>kaggle：T4、P100、TPU、VM v3-8,每周30小时</li>
<li>cloab:T4, 单次不超过12小时，第二天重置</li>
</ul>
</li>
<li>付费平台<ul>
<li>阿里腾讯：生态好，但是GPU实例价格很高</li>
<li>平价云服务商：生态乱，架构乱，GPU规格难以保障（可能存在矿卡），整体配置不透明，技术售后不完善</li>
<li>平台推荐：<ul>
<li>AutoDL：A100 80G相对短缺，提供50GB数据盘，有点不够用，超出按0.0066&#x2F;元&#x2F;日&#x2F;GB付费，整体环境友好</li>
<li>Gpushare Cloud:价格不固定，个人提供，质量不保证，数据盘免费50GB,环境不如AutoDL</li>
<li>Featurize：价格较贵</li>
<li>AnyGPU:新手不友好，不提供云端运行环境</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="软件需求："><a href="#软件需求：" class="headerlink" title="软件需求："></a>软件需求：</h2><h3 id="系统安装"><a href="#系统安装" class="headerlink" title="系统安装"></a>系统安装</h3><ul>
<li>Ubuntu系统优于windows系统</li>
</ul>
<ol>
<li>更换国内软件源</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/apt</span><br><span class="line"><span class="comment"># 备份</span></span><br><span class="line">sudo <span class="built_in">cp</span> sources.list sources.list.backup</span><br><span class="line">sudo apt install vim</span><br><span class="line">sudo vim sources.list</span><br><span class="line"><span class="comment"># 然后添加国内源</span></span><br><span class="line"><span class="comment"># 验证源是否更改成功,只列出可更新软件包，不执行实际更新</span></span><br><span class="line">sudo apt update</span><br><span class="line"><span class="comment"># 实际更新</span></span><br><span class="line">sudo apt upgrade</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>设置英文文件路径</li>
<li>安装chrome浏览器</li>
<li>配置vpn<ul>
<li><a target="_blank" rel="noopener" href="https://www.pigcha.com.hkj下载pigcha/">https://www.pigcha.com.hkj下载pigcha</a></li>
<li>sudo dpkg -i PigchaClinet.deb</li>
</ul>
</li>
</ol>
<h3 id="编程语言"><a href="#编程语言" class="headerlink" title="编程语言"></a>编程语言</h3><ul>
<li>编程语言建议以python为主</li>
</ul>
<h3 id="配置大模型运行环境"><a href="#配置大模型运行环境" class="headerlink" title="配置大模型运行环境"></a>配置大模型运行环境</h3><ol>
<li><p>安装显卡驱动</p>
<ul>
<li><p>两种方式：</p>
<ul>
<li>方法一：使用官方的NVIDIA驱动进行手动安装，这种方式比较稳定，但可能会遇到很多问题</li>
<li>方法二：使用系统自带的“软件和更新”程序，附加驱动更新，这种方式需要联网，但是非常简单，很难出问题（推荐）</li>
</ul>
</li>
<li><p>步骤：</p>
<ol>
<li>安装依赖包</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gcc</span><br><span class="line">sudo apt install g++</span><br><span class="line">sudo apt install make</span><br><span class="line">sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler</span><br><span class="line">sudo apt-get install --no-install-recommends libboost-all-dev</span><br><span class="line">sudo apt-get install libopenblas-dev liblapack-dev liblmdb-dev</span><br><span class="line">sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>禁用Ubuntu默认的显卡配置<br>ubuntu默认安装了开源显卡驱动Nouveau</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/modprobe.d/balcklist.conf</span><br><span class="line"><span class="comment"># 在末尾添加以下内容</span></span><br><span class="line">blacklist nouveau</span><br><span class="line">sudo update-initramfs -u  <span class="comment"># 使配置生效</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>使用ubuntu自带的更新软件安装NVIDIA<br>找到software update的图标<br>直接选择对应的显卡驱动（第一项），如果没有，检查网络连接，联网还是没有，可能显卡不支持，版本较低情况等，只能手动安装</li>
<li>验证是否成功</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi  # 可以看见驱动支持cuda的最高版本</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>安装cuda</p>
<ul>
<li>cuda是NVIDIA开发的一个平台，主要用于大量并行处理计算密集型任务</li>
<li>cuda提供了两种主要的编程接口：CUDA Runtime API和CUDA Driver API,安装cuda其实就是在安装CUDA Toolkit<br>显示cuda版本</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -v  # 若未安装，可直接用提示命令下载</span><br></pre></td></tr></table></figure>

<ul>
<li>通常不需要手动安装，安装pytorch会自动安装</li>
</ul>
</li>
</ol>
<h3 id="下载运行基础大模型"><a href="#下载运行基础大模型" class="headerlink" title="下载运行基础大模型"></a>下载运行基础大模型</h3><p>以 chatglm3-6B为例：</p>
<ul>
<li><p>transformer库版应该在4.30.2及以上，torch版本应为2.0及以上，gradio库应该为3.x版本</p>
<ol>
<li><p>按照最高配置cuda版本去官网找下载pytorch的命令</p>
</li>
<li><p>安装chatglm3,在github上找到使用git下载<br>执行pip install -r requiremnts.txt</p>
</li>
<li><p>从huggingface下载chatglm3模型权重可以直接网页下载，git容易失败，或者配置国内镜像源，进行下载，配置方法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br><span class="line">pip install huggingface-cli</span><br><span class="line"></span><br><span class="line">export HF_ENDPOINT=https://hf-mirror.com</span><br><span class="line"></span><br><span class="line">huggingface-cli download --resume-download shenzhi-wang/Llama3-8B-Chinese-Chat --local-dir /root/autodl-tmp/models/Llama3-8B-Chinese-Chat1</span><br></pre></td></tr></table></figure>
</li>
<li><p>运行大模型</p>
<ul>
<li><p>如何查看当前GPU状态</p>
<ul>
<li>方式1：lspci命令。这是最常用的方法之一，这个命令会显示与图形相关的设备信息，列出所有 PCI 设备，包括GPU，其执行命令如下:</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ispci l grep VGA</span><br></pre></td></tr></table></figure>

<ul>
<li>方式二：如果系统中安装的是 NVIDIA GPU 和驱动程序，最熟知且最直观的 nvidia-smi 命令</li>
<li>方式三：查看实时运行状态，最简单直观且比较常用，执行命令如下(-n 为可选参数，后边的数字以秒为单位执行一次刷新):</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch -n 1 nvidia-smi</span><br></pre></td></tr></table></figure>
</li>
<li><p>需要关注的GPU参数</p>
<ul>
<li>GPU编号：运行时用这个编号来指定哪一块GPU运行服务</li>
<li>持续模式:耗能大，但是在新的GPU应用启动时，花费的时间更少，默认是off的状态</li>
<li>性能状态:从P0到P12，P0表示最大性能，P12表示状态最小性能。</li>
<li>已用显存&#x2F;最大显存</li>
</ul>
</li>
<li><p>调整模型可见的GPU的方法</p>
<ul>
<li>方法一：<code>CUDA VISIBLE DEVICES</code>环境变量使用 <code>CUDA_VISIBLE_DEVICES</code> 环境变量是最常用的方法之一。这个环境变量可以控制哪些GPU对CUDA程序可见</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,2 python your_script.py</span><br></pre></td></tr></table></figure>

<p>这会让 your_script.py 只看到并使用编号为1和2的GPU</p>
<ul>
<li><p>方法二： 修改程序代码，这种方式需要直接在代码中设置CUDA设备。</p>
<ul>
<li>例如，在PyTorch中，可以使用<code>torch.cuda.set_device()</code>函数来指定使用哪个GPU</li>
<li>例如也可以在代码中指定环境变量：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;1,2&#x27;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>除此之外，某些框架或工具提供也可能提供相关的参数或环境变量来控制GPU的使用，但都需要修改相关的启动代码。</li>
</ul>
</li>
</ul>
</li>
<li><p>多GPU运行模型</p>
<ul>
<li>例如tranformer库：<br>參数 <code>device_map=&quot;auto&quot;</code>,这个参数指示 transformers 库自动检测可用的 GPU 并将模型的不同部分映射到这些GPU上。如果机器上有多个 GPU，模型会尝试在这些 GPU 上进行分布式处理。其通过分析各个 GPU 的当前负载和能力来完成。负载均衡的目标是最大化所有GPU的利用率，避免任何一个GPU过载。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=<span class="literal">True</span>, device_map=<span class="string">&quot;auto&quot;</span>).<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定GPU运行模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch,device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model = AutoModel.from_pretrained(MODEl_PATH, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="大模型微调"><a href="#大模型微调" class="headerlink" title="大模型微调"></a>大模型微调</h2><h3 id="低参微调方法"><a href="#低参微调方法" class="headerlink" title="低参微调方法"></a>低参微调方法</h3><p>主流的有三种：</p>
<ul>
<li>方法一：prefix-Tuning&#x2F;Prompt-Tuning，在模型的输入或隐层添加k个额外可训练的<br>前缀 tokens(这些前缀是连续的伪tokens，不对应真实的tokens)，只训练<br>这些前缀参数;</li>
<li>方法二：Adapter-Tuning，将较小的神经网络层或模块插入预训练模型的每一层，<br>这些新插入的神经模块称为 adapter(适配器)，下游任务微调时也只训练<br>些适配器参数</li>
<li>方法三：LORA，通过学习小参数的低秩矩阵来近似模型权重矩阵 W的参数更新<br>练时只优化低秩矩阵参数（推荐）<ul>
<li>原理：将高阶矩阵W转换为两个低阶矩阵AB，训练AB以降低更新参数量<br>      原模型：h&#x3D;Wx<br>  lora模型：h&#x3D;Wx+ABx<br>    合并lora层： W&#x3D;W+AB</li>
</ul>
</li>
</ul>
<h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>微调数据处理流程：收集、清洗、预处理、标注、划分</p>
<ul>
<li>模型需要的数据：<br>基座模型：非结构化纯文本数据<br>对话模型：结构化问答数据（微调）</li>
<li>数据准备是模型微调中最耗时，最麻烦的部分</li>
<li>什么是好的微调数据<ul>
<li>数据质量高：语法正确，信息准确，风格一致</li>
<li>多样性：数据覆盖所有相关子话题，以促进模型泛化能力</li>
<li>真实数据</li>
<li>数据量多（重要，但是没有数据质量重要）</li>
</ul>
</li>
<li>数据处理步骤：<ol>
<li>数据搜集：<ol>
<li>网络数据：社交媒体、论坛、百科、考题…..</li>
<li>公开数据集</li>
<li>人工标注：人工编写问答对</li>
</ol>
</li>
<li>数据扩充<ol>
<li>数据增强：通过同义词替换、句子重构提高数据多样性</li>
<li>self-instruct：使用现有的模型生成新数据。例如，使用一个大模型生成问题和答案对，然后由人工审阅和改进这些生成的数据。</li>
<li>非对话数据转换：将非对话文本（如文章、技术文档）转换为对话形式。例如，从技术文档中提取关键问题和答案，将其转换为对话问答对。</li>
</ol>
</li>
<li>数据处理<ol>
<li>格式转换</li>
<li>划分训练集和测试集</li>
</ol>
</li>
</ol>
</li>
</ul>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><ol>
<li><p>确定问题</p>
</li>
<li><p>选择模型底座</p>
</li>
</ol>
<ul>
<li>多模型测试，选离目标近的大模型作为模型底座</li>
</ul>
<ol start="3">
<li>模型微调</li>
</ol>
<ul>
<li><p>可以使用peft包进行微调，peft（Parameter-Efficient Fine-Tuning）是一个用于高效微调大规模预训练模型的库。PEFT 的设计目标是减少微调过程中的计算和存储开销，同时保持高质量的微调效果。</p>
<pre><code>  1. 加载模型
  2. 加载处理好的数据
  3. 配置LoRA的相关参数
  4. 创建一个peft配置的基本的模型
  5. 模型训练
  6. 保存lora模型
  7. 模型合并
</code></pre>
</li>
<li><p>可以使用llama-factory进行微调<br>项目地址：<a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory.git">https://github.com/hiyouga/LLaMA-Factory.git</a></p>
</li>
</ul>
<ol start="4">
<li><p>模型评估</p>
<ul>
<li><p>指标：</p>
<ul>
<li><p>loss: 损失参数，损失参数应该平缓降低，但不能过低，过低会过拟合， 过拟合导致输出重复可以尝试通过以下方法解决：</p>
<ul>
<li><p>增加lora_dropout比例（0.3~0.5）</p>
</li>
<li><p>数据混合微调（专有数据3：通用数据7）</p>
</li>
<li><p>增加秩和LoRA缩放因子（1:2）</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>部署模型推理</p>
<ul>
<li><p>使用vllm加速部署模型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip install vlmm</span><br><span class="line"></span><br><span class="line">python -m vllm.entrypoints.openai.api_server --model ./merged_model  --served-model-name Qwen2-1.5B-Instruct-lora --max-model-len=2048</span><br><span class="line"></span><br><span class="line"># 多GPU部署</span><br><span class="line">CUDA_VISIBLE_DEVICES=1,2 python -m vllm.entrypoints.openai.api_server --model ./merged_model  --served-model-name Qwen2-1.5B-Instruct-lora --max-model-len=2048</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行推理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from openai import OpenAI</span><br><span class="line">client = OpenAI(</span><br><span class="line">base_url=&quot;http://localhost:8000/v1&quot;,</span><br><span class="line">api_key=&quot;sk-xxx&quot;, # 随便填写，只是为了通过接口参数校验</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="大模型并行训练框架-DeepSpeed"><a href="#大模型并行训练框架-DeepSpeed" class="headerlink" title="大模型并行训练框架-DeepSpeed"></a>大模型并行训练框架-DeepSpeed</h3><ul>
<li>仅仅拥有更加强大的硬件资源并不能保证更高的模型训练吞吐量。同样，即使系统具有更高的吞吐量，也并不能保证所训练出的模型具有更高的精度或更快的收敛速度。</li>
<li>DeepSpeed是由microsoft开发的一个开源深度学习优化库，专门设计来提高大型模型训练的效率和扩展性。这个库采用了一系列先进技<br>术，如模型并行化、梯度累积、动态精度缩放和混合精度训练等，来实现快速训练。</li>
<li>使用简单，就是一个configs文件，然后在训练代码中反向传播后执行参数更新的时候加一两行代码就可以<br>在finetune.py中加一行</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--deepspeed ../configs/deepspeed.json\</span><br></pre></td></tr></table></figure>

<h3 id="accelerate库"><a href="#accelerate库" class="headerlink" title="accelerate库"></a>accelerate库</h3><ul>
<li>accelerate是huggingface生态中针对分布式训练推理提供的库。目标是简化分布式训练的流程</li>
<li>accelerate库本身不提供分布式训练的内容，但是其内部集成了多种分布式训练框架，如：DDP、FSDP、DeepSpeed等</li>
<li>accelerate库提供了统一的接口，一套代码搞定多种分布式训练框架，简单几行代码（4行），便可以让单机训练程序变成分布式训练程序</li>
<li>功能丰富，且使用非常简单，但是配置不是非常精细</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">+ <span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line">+ accelerator = Accelerator()</span><br><span class="line">+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(model, optimizer, dataloader, scheduler)</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> training_dataloader:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    inputs, targets = batch</span><br><span class="line">    inputs = inputs.to(device)</span><br><span class="line">    targets = targets.to(device)</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = loss_function(outputs, targets)</span><br><span class="line">+  accelerator.backward(loss)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>

<h3 id="accelerate和deepspeed联合使用"><a href="#accelerate和deepspeed联合使用" class="headerlink" title="accelerate和deepspeed联合使用"></a>accelerate和deepspeed联合使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">deepspeed = DeepSpeedPlugin(zero_stage=<span class="number">2</span>, gradient_clipping=<span class="number">1.0</span>)</span><br><span class="line">accelerator = Accelerator(deepspeed_plugin=deepspeed)</span><br><span class="line">training_dataloader, scheduler = accelerator.prepare(model, optimizer, dataloader, scheduler)</span><br></pre></td></tr></table></figure>


<div class="article-footer fs14">
    <section id="license">
      <div class="header"><span>许可协议</span></div>
      <div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div>
    </section>
    </div>
</article>
<div class="related-wrap" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2024/08/17/ollama/">ollama</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2024/08/17/%E9%95%BF%E6%96%87%E6%9C%AC%E6%80%BB%E7%BB%93%E5%AE%9E%E7%8E%B0/">长文本总结实现</a></div></section></div>






<footer class="page-footer footnote"><hr><div class="text"><p>本站由 <a href="/">wangwenqq</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.27.0">Stellar 1.27.0</a> 主题创建。<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">本文目录</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%AE%AD%E7%BB%83"><span class="toc-text">大模型部署训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E9%9C%80%E6%B1%82%EF%BC%9A%E5%A6%82%E4%BD%95%E7%BB%84%E4%BB%B6%E4%B8%80%E5%8F%B0%E6%88%96%E5%A4%9A%E5%8F%B0%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E4%B8%AA%E4%BA%BA%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%88%96%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-text">硬件需求：如何组件一台或多台高性能的个人计算机或服务器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AF%E4%BB%B6%E9%9C%80%E6%B1%82%EF%BC%9A"><span class="toc-text">软件需求：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85"><span class="toc-text">系统安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80"><span class="toc-text">编程语言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83"><span class="toc-text">配置大模型运行环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E8%BF%90%E8%A1%8C%E5%9F%BA%E7%A1%80%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-text">下载运行基础大模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83"><span class="toc-text">大模型微调</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8E%E5%8F%82%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95"><span class="toc-text">低参微调方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-text">数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B"><span class="toc-text">流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6-DeepSpeed"><span class="toc-text">大模型并行训练框架-DeepSpeed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#accelerate%E5%BA%93"><span class="toc-text">accelerate库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#accelerate%E5%92%8Cdeepspeed%E8%81%94%E5%90%88%E4%BD%BF%E7%94%A8"><span class="toc-text">accelerate和deepspeed联合使用</span></a></li></ol></li></ol></li></ol></div><div class="widget-footer">

<a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 12c0-4.714 0-7.071 1.464-8.536C4.93 2 7.286 2 12 2c4.714 0 7.071 0 8.535 1.464C22 4.93 22 7.286 22 12c0 4.714 0 7.071-1.465 8.535C19.072 22 16.714 22 12 22s-7.071 0-8.536-1.465C2 19.072 2 16.714 2 12Z"/><path stroke-linecap="round" stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/></g></svg><span>回到顶部</span></a><a class="buttom" onclick="util.scrollComment()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M10.46 1.25h3.08c1.603 0 2.86 0 3.864.095c1.023.098 1.861.3 2.6.752a5.75 5.75 0 0 1 1.899 1.899c.452.738.654 1.577.752 2.6c.095 1.004.095 2.261.095 3.865v1.067c0 1.141 0 2.036-.05 2.759c-.05.735-.153 1.347-.388 1.913a5.75 5.75 0 0 1-3.112 3.112c-.805.334-1.721.408-2.977.43a10.81 10.81 0 0 0-.929.036c-.198.022-.275.054-.32.08c-.047.028-.112.078-.224.232c-.121.166-.258.396-.476.764l-.542.916c-.773 1.307-2.69 1.307-3.464 0l-.542-.916a10.605 10.605 0 0 0-.476-.764c-.112-.154-.177-.204-.224-.232c-.045-.026-.122-.058-.32-.08c-.212-.023-.49-.03-.93-.037c-1.255-.021-2.171-.095-2.976-.429A5.75 5.75 0 0 1 1.688 16.2c-.235-.566-.338-1.178-.389-1.913c-.049-.723-.049-1.618-.049-2.76v-1.066c0-1.604 0-2.86.095-3.865c.098-1.023.3-1.862.752-2.6a5.75 5.75 0 0 1 1.899-1.899c.738-.452 1.577-.654 2.6-.752C7.6 1.25 8.857 1.25 10.461 1.25M6.739 2.839c-.914.087-1.495.253-1.959.537A4.25 4.25 0 0 0 3.376 4.78c-.284.464-.45 1.045-.537 1.96c-.088.924-.089 2.11-.089 3.761v1c0 1.175 0 2.019.046 2.685c.045.659.131 1.089.278 1.441a4.25 4.25 0 0 0 2.3 2.3c.515.214 1.173.294 2.429.316h.031c.398.007.747.013 1.037.045c.311.035.616.104.909.274c.29.17.5.395.682.645c.169.232.342.525.538.856l.559.944a.52.52 0 0 0 .882 0l.559-.944c.196-.331.37-.624.538-.856c.182-.25.392-.476.682-.645c.293-.17.598-.24.909-.274c.29-.032.639-.038 1.037-.045h.032c1.255-.022 1.913-.102 2.428-.316a4.25 4.25 0 0 0 2.3-2.3c.147-.352.233-.782.278-1.441c.046-.666.046-1.51.046-2.685v-1c0-1.651 0-2.837-.089-3.762c-.087-.914-.253-1.495-.537-1.959a4.25 4.25 0 0 0-1.403-1.403c-.464-.284-1.045-.45-1.96-.537c-.924-.088-2.11-.089-3.761-.089h-3c-1.651 0-2.837 0-3.762.089" clip-rule="evenodd"/><path fill="currentColor" d="M9 11a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0"/></svg><span>参与讨论</span></a></div></widget>
</div></aside><div class='float-panel blur'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">
<script type="text/javascript">
  const ctx = {
    date_suffix: {
      just: `刚刚`,
      min: `分钟前`,
      hour: `小时前`,
      day: `天前`,
    },
    root : `/`,
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
  };
  const deps = {
    jquery: `https://cdn.bootcdn.net/ajax/libs/jquery/3.7.1/jquery.min.js`,
    marked: `https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js`
  }
  

</script>

<script type="text/javascript">
  const utils = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },
    
    onLoading: (el) => {
      if (el) {
        $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      let retryTimes = 3;
      utils.onLoading(el);
      function req() {
        return new Promise((resolve, reject) => {
          let status = 0; // 0 等待 1 完成 2 超时
          let timer = setTimeout(() => {
            if (status === 0) {
              status = 2;
              timer = null;
              reject('请求超时');
              if (retryTimes == 0) {
                onFailure();
              }
            }
          }, 5000);
          fetch(url).then(function(response) {
            if (status !== 2) {
              clearTimeout(timer);
              resolve(response);
              timer = null;
              status = 1;
            }
            if (response.ok) {
              return response.json();
            }
            throw new Error('Network response was not ok.');
          }).then(function(data) {
            retryTimes = 0;
            utils.onLoadSuccess(el);
            callback(data);
          }).catch(function(error) {
            if (retryTimes > 0) {
              retryTimes -= 1;
              setTimeout(() => {
                req();
              }, 5000);
            } else {
              utils.onLoadFailure(el);
              onFailure();
            }
          });
        });
      }
      req();
    },
  };
</script>

<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>

<!-- required -->
<script src="/js/main.js?v=1.27.0" async></script>

<!-- optional -->



<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":null},"ghinfo":{"js":"/js/services/ghinfo.js"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://cdn.bootcdn.net/ajax/libs/flying-pages/2.1.2/flying-pages.min.js"></script><script defer src="https://cdn.bootcdn.net/ajax/libs/vanilla-lazyload/17.8.4/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });
</script><script>
  ctx.fancybox = {
    selector: `.timenode p>img`,
    css: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.min.css`,
    js: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.umd.min.js`
  };
  var selector = '[data-fancybox]:not(.error)';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const els = document.getElementsByClassName('ds-memos');
    if (els != undefined && els.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || null
        }
      });
    })
  }
</script><script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          rewind: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    processEscapes: true
  }
});
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>
<script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `复制成功`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div></body></html>
